{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "from mistralai import Mistral\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "# Load API key from .env file (create this file with your API key)\n",
    "MISTRAL_API_KEY = \"6X71LCz0rZckardeNbmor17ONKFO1NVS\"  # Replace with your Mistral API key\n",
    "api_key = MISTRAL_API_KEY\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "document_url = \"https://github.com/jvankooo/ocrpro/raw/main/Phable%20CAM%20Final.pdf\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_response = client.ocr.process(\n",
    "    model=\"mistral-ocr-latest\",\n",
    "    document={\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": document_url\n",
    "    },\n",
    "    include_image_base64=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR response saved to ocr_response.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chira\\AppData\\Local\\Temp\\ipykernel_24340\\1437944098.py:7: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  json.dump(ocr_response.dict(), f, indent=4, ensure_ascii=False)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save the OCR response to a JSON file\n",
    "ocr_json_path = \"ocr_response.json\"\n",
    "\n",
    "with open(ocr_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ocr_response.dict(), f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"OCR response saved to {ocr_json_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR response loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load the OCR response JSON\n",
    "ocr_json_path = \"ocr_response.json\"\n",
    "\n",
    "with open(ocr_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    ocr_data = json.load(f)\n",
    "\n",
    "print(\"OCR response loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content saved to outputs/text_output.txt\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "text_file_path = \"outputs/text_output.txt\"\n",
    "\n",
    "with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for page in ocr_data[\"pages\"]:\n",
    "        f.write(f\"Page {page['index'] + 1}:\\n\")\n",
    "        f.write(page[\"markdown\"] + \"\\n\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "print(f\"Text content saved to {text_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0 has a table-like structure:\n",
      "| Proposal Summary |  | December, 2021 |  |\n",
      "| :-- | :--: | :--: | :--: |\n",
      "| Borrower Name: Terrals Technologies Pvt. Ltd. |  |  |  |\n",
      "| Sector: Chronic Disease Management and Healthcare Services Platform |  |  |  |\n",
      "| Proposed Amount: Up to INR 45,00,00,000 |  |  |  |\n",
      "| Tranche 1: INR 10,00,00,000 |  |  |  |\n",
      "| Tranche 2: Up to 15\\% of the equity raise (Series B) with an overall cap of INR 35Cr |  |  |  |\n",
      "| Tenor: 18 months from the date of disbursement of each tranche |  |  |  |\n",
      "| Debt XIRR: 16.56\\%; Warrants: 7\\%; XIRR with warrant upside: 31.2\\% |  |  |  |\n",
      "| Repayment Schedule: |  |  |  |\n",
      "| Quarterly Amortization (5 instalments); Moratorium for three months i.e., first principal instalment at the end of 6 |  |  |  |\n",
      "| months. The coupons are to be serviced monthly on the last day of each month. |  |  |  |\n",
      "| Filtering Criteria: |  |  |  |\n",
      "| Criteria | Stride Filter | Pre-Equity | Post-Equity* |\n",
      "| Equity Raised till Date | $>$ INR 50Cr | INR 64Cr | INR 192 Cr |\n",
      "| Debt/Equity Raised (incl.\n",
      "================================================================================\n",
      "Page 1 has a table-like structure:\n",
      "> Company to share all notices, board minutes \\& other necessary board meeting material to Stride Ventures within 7 days from such board meeting.\n",
      "\n",
      "# Partly Paid CCPS Valuation: \n",
      "\n",
      "Investor shall be entitled to subscribe to partly paid-up shares to the extent of $7 \\%$ of the Investment Amount, which when exercised at any time during the Exercise Period ( 5 years from the Maturity date of the last tranche or end of the fund life, whichever is later.), shall convert into equivalent equity shares of the Borrower at par to the per share price of Series B i.e. $\\mathbf{2 0 \\%}$ discount to Series C.\n",
      "\n",
      "## Promoters \\& Cap Table\n",
      "\n",
      "## Shareholding Pattern:\n",
      "\n",
      "Shareholding Pattern of Terrals Technologies Pvt. Ltd (Pre-Series B):\n",
      "\n",
      "| Institution / Individual | Investor <br> Since | Fund Details | Shareholding |\n",
      "| :-- | :--: | :-- | :--: |\n",
      "| Founders/Promoters |  |  |  |\n",
      "| Sumit Sinha | 2018 |  | $19.84 \\%$ |\n",
      "| Mukesh Bansal | 2018 |  | $14.38 \\%$ |\n",
      "| Institutions |  |  |  |\n",
      "| Kalaari Capital | 2021 | \n",
      "================================================================================\n",
      "Page 2 has a table-like structure:\n",
      "![img-1.jpeg](img-1.jpeg)\n",
      "\n",
      "# Organisation Chart: \n",
      "\n",
      "![img-2.jpeg](img-2.jpeg)\n",
      "\n",
      "Key Management Personnel: $\\rightarrow$ Taun Lache weak in bome 1 stevent aprience\n",
      "\n",
      "| Key Personnel | Background Details |\n",
      "| :--: | :--: |\n",
      "| Venkatesh Walajabad <br> Engineering Manager | Professional Background (14+ years, 1+ years with Phable) <br> $>$ Strategic Solutions Group, Independent Consultant ('19-'20) <br> $>$ LogN/SchoolMint, Director Of Engineering ('13- '19) <br> $>$ Apollo Group, Senior Software Engineer ('10- '13) <br> $>$ IG InfoTech, Senior Software Developer ('08- '10) <br> $>$ Apollo Group, IT Engineer II ('07- '08) <br> $>$ Brahma Consulting Services, Programmer Analyst ('05-'07) <br> Educational Background: <br> $>$ M.S. Electrical Engineering <br> Southern Illinois University, Carbondale (Class of 2004) <br> $>$ B.E. Electrical \\& Electronics Engineering <br> Vellore Institute of Technology (Class of 2002) |\n",
      "| ![img-3.jpeg](img-3.jpeg) | Professional Background (20+ years, less than a \n",
      "================================================================================\n",
      "Page 4 has a table-like structure:\n",
      "![img-5.jpeg](img-5.jpeg)\n",
      "\n",
      "# Board Composition: \n",
      "\n",
      "Terrals Technologies Pvt. Ltd. has appointed four directors:\n",
      "\n",
      "- Sumit Sinha, Co-Founder \\& CEO - Phable\n",
      "- Mukesh Kumar Bansal, Co-Founder - Phable\n",
      "- Mitesh Daga, Managing Director - TPG Global \\& Board Member - Manipal Health Enterprises Pvt. Ltd.\n",
      "- William Bao Bean, General Partner - SOSV\n",
      "\n",
      "\n",
      "## Employee split:\n",
      "\n",
      "| Department | \\# of Employees |\n",
      "| :-- | :--: |\n",
      "| Founders | 2 |\n",
      "| Finance | 6 |\n",
      "| Digital | 14 |\n",
      "| Product | 16 |\n",
      "| Management Office | 21 |\n",
      "| Doctor Onboarding Operations | 25 |\n",
      "| Technology | 30 |\n",
      "| Revenue Operations | 126 |\n",
      "| Grand Total | $\\mathbf{2 4 0}$ |\n",
      "================================================================================\n",
      "Page 5 has a table-like structure:\n",
      "# Equity Funding till date and Valuation movement: \n",
      "\n",
      "| Funding <br> Date | Round | Investment <br> Amount <br> (INR Cr) | Post Money <br> Valuation <br> (INR Cr) | Investors |\n",
      "| :--: | :--: | :--: | :--: | :--: |\n",
      "| Jan-21 | Series A | 50.5 | 150.0 | Kalaari Capital, SOSV, Fresco, Manipal <br> Hospitals, Omron, Social Starts |\n",
      "| Jan-20 | Seed | 10.3 | 45.0 | SOSV, Betatron, Fresco, Lets Venture, <br> Social Starts, Inflection |\n",
      "| Aug-19 | Seed | 1.9 | 19.0 | SOSV |\n",
      "| Jul-18 | Seed | 2.1 | 10.0 | Omphalos Ventures LLP |\n",
      "| TOTAL |  | 64 |  |  |\n",
      "\n",
      "Note: The company is raising Series B of $\\$ 17 \\mathrm{Mn}$ (INR 128 Cr ) with infusion expected in Dec'21. Second closing of this round is expected by the end of January 2022 \\& may go as high as $\\$ 40 \\mathrm{Mn}$ cumulatively. The current Series B round will be a convertible round valued at $20 \\%$ discount to series C (expected to close Q1 FY23).\n",
      "\n",
      "## Key Investors:\n",
      "\n",
      "## Kalaari Capital:\n",
      "\n",
      "Founded in 2006 by Vani Kola, Kalaari Capital is an Ear\n",
      "================================================================================\n",
      "Page 6 has a table-like structure:\n",
      "# SOSV: \n",
      "\n",
      "Founded in 1995, by company's Managing Partner Sean O'Sullivan (following the IPO of MapInfo, a startup he co-founded that pioneered computer-based street mapping), SOSV is a global venture capital firm that operates early stage startup development programs.\n",
      "\n",
      "SOSV's programs are focused on two broad areas. The first is revolutionary technology that promises the betterment of humanity and the planet. The second is cross-border markets, notably in Asia, that are ripe for explosive growth. SOSV joins seed, series A and later rounds while providing founders with ongoing support and community.\n",
      "\n",
      "SOSV's programs include hardware-oriented HAX and life-science driven IndieBio. Both offer deep technical expertise on-site as well as well-equipped lab and fabrication facilities. At Chinaccelerator and MOX, market-entry and product development experts help entrepreneurs win cross-border market opportunities across Asia. SOSV's latest program, dlab, fosters new players in the blockchain ec\n",
      "================================================================================\n",
      "Page 7 has a table-like structure:\n",
      "| SNo. | Company Name | Founded <br> Year | City | Total Funding (INR Crores) | Company <br> Stage | Latest Valuation (INR Crores) |\n",
      "| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
      "| 1 | Tricog | 2015 | Bangalore | 130 | Series B | 450 |\n",
      "| 2 | CellMax Life | 2012 | Ahmedabad | - | Funding Raised | - |\n",
      "| 3 | Singlife | 2014 | Singapore | 1,395 | Series C | - |\n",
      "| 4 | Craif | 2018 | Japan | 85 | Series B | - |\n",
      "| 5 | Betterment | 2010 | USA | 2,513 | Series F | - |\n",
      "| 6 | Unifa | 2015 | Japan | 517 | Series D | - |\n",
      "| 7 | Chikaku | 2014 | Japan | 44 | Series A | - |\n",
      "| 8 | Pagaya | 2004 | USA | 1,095 | Series D | - |\n",
      "| 9 | ACT Genomics | 2014 | Taiwan | 154 | Series B | - |\n",
      "| 10 | Kakehashi | 2016 | Japan | 225 | Series B | - |\n",
      "\n",
      "# Strategic Investors: \n",
      "\n",
      "## Manipal Hospitals:\n",
      "\n",
      "Founded in 1953 as a part of Manipal Education System, Manipal Hospital is spread across 15 locations in India. The first branch of Manipal Hospitals was started in 1991 in Bengaluru. The branch is a 600 -bed Quaterna\n",
      "================================================================================\n",
      "Page 12 has a table-like structure:\n",
      "Growth in Users\n",
      "(INR lakh)\n",
      "![img-11.jpeg](img-11.jpeg)\n",
      "\n",
      "|  | Apr'21 | May'21 | Jun'21 | Jul'21 | Aug'21 |\n",
      "| :--: | :--: | :--: | :--: | :--: | :--: |\n",
      "| Total Users (in <br> Lakhs) | 4.02 | 4.67 | 5.42 | 6.25 | 7.41 |\n",
      "| Monthly Active <br> Patients | $69 \\%$ | $64 \\%$ | $64 \\%$ | $64 \\%$ | $63 \\%$ |\n",
      "| Transacting users <br> (\\% of MAU) | $4 \\%$ | $4 \\%$ | $3 \\%$ | $3 \\%$ | $4 \\%$ |\n",
      "\n",
      "Note: Monthly active users are defined as users having at least one session on the app per month.\n",
      "\n",
      "# Number of Transactions and AOV Chart MoM \n",
      "\n",
      "![img-12.jpeg](img-12.jpeg)\n",
      "\n",
      "The chronic disease patients initially use Phable for ongoing engagement with their doctors, documenting health records, and monitoring their vitals through smart devices. This drives change in consumer behavior as users begin to transact on the app (medicines, lab bookings, consultations, etc.) in addition to using the app merely for doctor engagement and tracking. This changing consumer behavior is captured in the growing user base who \n",
      "================================================================================\n",
      "Page 13 has a table-like structure:\n",
      "# Low Customer CAC due to Doctor Acquisition Channel \n",
      "\n",
      "Acquiring patients through doctors have created a strong network effect amongst patients and the doctor community alike. This customer acquisition approach has led to consistent drop in CAC across doctors and patients:\n",
      "\n",
      "| Particulars <br> (INR) | Jun- <br> 20 | Jul- <br> 20 | Aug- <br> 20 | Sep- <br> 20 | Oct- <br> 20 | Nov- <br> 20 | Dec- <br> 20 | Jan- <br> 21 | Feb- <br> 21 | Mar- <br> 21 | Apr- <br> 21 | May- <br> 21 | Jun- <br> 21 | Jul- <br> 21 | Aug- <br> 21 |\n",
      "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
      "| Doctor <br> CAC | 18,244 | 6,763 | 3,562 | 3,243 | 2,757 | 4,550 | 5,483 | 6,759 | 2,956 | 7,669 | 4,308 | 4,407 | 4,182 | 3,483 | 3,351 |\n",
      "| User CAC* | 241 | 154 | 83 | 93 | 135 | 75 | 36 | 56 | 35 | 41 | 50 | 24 | 30 | 39 | 45 |\n",
      "\n",
      "*Acquired users are total sign ups on the platform\n",
      "In 15 months, the CAC per doctor and user has come down from INR 18,244 an\n",
      "================================================================================\n",
      "Page 14 has a table-like structure:\n",
      "![img-13.jpeg](img-13.jpeg)\n",
      "\n",
      "Note: Active users are defined as users having at least one session on the app per day, week and month\n",
      "\n",
      "While monthly active users have been stagnant at $\\sim 1.1$ lakh, weekly active users have gone up by $\\sim 40 \\%$ to $\\sim 44,000$ while daily active users have gone up by $\\sim 95 \\%$ to $\\sim 31,000$. This is a strong indicator of increase in adoption by the active users over time:\n",
      "\n",
      "| Particulars | Nov- <br> 20 | Dec- <br> 20 | Jan- <br> 21 | Feb- <br> 21 | Mar- <br> 21 | Apr- <br> 21 | May- <br> 21 | Jun- <br> 21 | Jul- <br> 21 | Aug- <br> 21 | Sep- <br> 21 | Oct- <br> 21 |\n",
      "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
      "| DAU/MAU | $14 \\%$ | $11 \\%$ | $21 \\%$ | $18 \\%$ | $21 \\%$ | $23 \\%$ | $23 \\%$ | $20 \\%$ | $23 \\%$ | $26 \\%$ | $27 \\%$ | $28 \\%$ |\n",
      "| WAU/MAU | $27 \\%$ | $27 \\%$ | $44 \\%$ | $31 \\%$ | $29 \\%$ | $29 \\%$ | $31 \\%$ | $30 \\%$ | $33 \\%$ | $37 \\%$ | $37 \\%$ | $40 \\%$ |\n",
      "\n",
      "Customer Ratings Analysis\n",
      "\n",
      "================================================================================\n",
      "Page 15 has a table-like structure:\n",
      "Phable has a favourable online rating compared to most of the existing Medicare and healthcare segments. The number downloads are more than 1 million and $\\sim 8,500$ have given their reviews to aggregate a cumulative of 4.2 on Google Play Store. Even on Glassdoor, $\\sim 80 \\%$ of employees recommend it as a good place of work.\n",
      "\n",
      "# Supply Partners \n",
      "\n",
      "Phable has created a network of healthcare partners across doctors, hospitals, device manufacturers, consumables, etc. to cater to comprehensive healthcare requirements of all users on the platform. For doctors, the Phable platform serves as a clinic management software to manage bookings, tele-consultations, e-bookings, billing, etc. which serves as a hook for doctors to adopt Phable.\n",
      "![img-15.jpeg](img-15.jpeg)\n",
      "\n",
      "## Doctor Acquisition Strategy\n",
      "\n",
      "The platform relies on doctor partnerships to acquire customers and has $\\sim 6,400$ doctor partners on its platform. Doctors are acquired mostly through an on-ground sales team (feet on street) and \n",
      "================================================================================\n",
      "Page 16 has a table-like structure:\n",
      "| Dr. Vivek Baliga | Years of Experience: 16 years <br> Practice Area: Internal Medicine and Cardiologist <br> Work Experience: <br> > Baliga Diagnostics, Bangalore <br> $>$ Aster RV Hospital, Bangalore <br> $>$ Fortis Hospital, Bangalore <br> $>$ HeartSense, Bangalore <br> $>$ Indian Academy of Echocardiography, Karnataka Chapter <br> Educational Qualifications: <br> $>$ MBBS, Kasturba Medical College, Mangalore (Class of 2000) <br> $>$ MRCP, Royal College of Physicians (Class of 2006) |\n",
      "| :--: | :--: |\n",
      "| Dr. Shrideep Arun Parab | Years of Experience: 20 years <br> Practice Area: Gynaecologist <br> Work Experience: <br> > Adishri Clinic, Pune <br> $>$ Nandadeep Hospital, Satara <br> Educational Qualifications: <br> $>$ MBBS, Satarov State Medical University, Russia (Class of 2010) <br> $>$ DGO, CPS Mumbai (Class of 2016) |\n",
      "| Dr. Venkatesh Somalaram | Years of Experience: 5 years <br> Practice Area: Cardiologist <br> Work Experience: <br> $>$ Aster RV Hospital, Bangalore <br> $>$ Forti\n",
      "================================================================================\n",
      "Page 17 has a table-like structure:\n",
      "In the last 12 months, Phable has doubled its doctor base by adding $\\sim 3,300$ doctors. $\\sim 65 \\%$ of the doctors added in Nov'20 continue to work with Phable as on Oct'21:\n",
      "\n",
      "| Month | \\#Doctors | Nov- <br> 20 | Dec- <br> 20 | Jan- <br> 21 | Feb- <br> 21 | Mar- <br> 21 | Apr- <br> 21 | May- <br> 21 | Jun- <br> 21 | Jul- <br> 21 | Aug- <br> 21 | Sep- <br> 21 | Oct- <br> 21 |\n",
      "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
      "| Nov-20 | 169 | $100 \\%$ | $85 \\%$ | $78 \\%$ | $73 \\%$ | $68 \\%$ | $68 \\%$ | $68 \\%$ | $68 \\%$ | $68 \\%$ | $68 \\%$ | $65 \\%$ | $65 \\%$ |\n",
      "| Dec-20 | 121 |  | $100 \\%$ | $87 \\%$ | $75 \\%$ | $74 \\%$ | $73 \\%$ | $73 \\%$ | $72 \\%$ | $64 \\%$ | $64 \\%$ | $64 \\%$ | $64 \\%$ |\n",
      "| Jan-21 | 85 |  |  | $100 \\%$ | $89 \\%$ | $84 \\%$ | $84 \\%$ | $84 \\%$ | $81 \\%$ | $81 \\%$ | $79 \\%$ | $74 \\%$ | $69 \\%$ |\n",
      "| Feb-21 | 105 |  |  |  | $100 \\%$ | $93 \\%$ | $90 \\%$ | $90 \\%$ | $85 \\%$ | $85 \\%$ | $81 \\%$ | $75 \\%$ | $71 \\%$ |\n",
      "| Mar-21 | \n",
      "================================================================================\n",
      "Page 18 has a table-like structure:\n",
      "|  |  | - Devices include - BP Monitor, Nebulizer, Weight \\& BMI, TH, TENS, Pulse Oximeter | > Sample blood tests (Blood, Saliva, etc.) <br> for users placing orders on Phable's platform |\n",
      "| :--: | :--: | :--: | :--: |\n",
      "| Revenue Sharing | - For every completed order, Phable to receive 5\\% commission on all delivered orders <br> - Customer Discount $=$ Pharmeasy base discount + 2\\% Phable Partnership discount | - Device Margin: $37 \\%$ to $102 \\%$ on cost price (subject to device type); Device cost ranges from INR 1,000 to INR 8,000 <br> - Subscriptions Margin: $10 \\%$ on first-time subscription; $10 \\%$ on renewal of subscription on cost price; First Subscription; INR 2,999 <br> Renewal: INR 1,599 | - Prima agrees to pay Phable the following fee: <br> > Pathology and Radiology <br> - <2 Lakhs - 40\\% <br> - 2-4 Lakhs - 45\\% <br> $->4$ Lakhs - 50\\% <br> > Packages <br> - < 30 pkgs - 30\\% <br> $->30$ pkgs - 35\\% <br> Home Collection Charges between INR 100-500 to be levied by Prima Diagno\n",
      "================================================================================\n",
      "Page 19 has a table-like structure:\n",
      "| Pharmeasy | Commission | - | - | - | - | 0.51 | 0.51 | $28.1 \\%$ |\n",
      "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
      "| 1 MG | Commission | - | 0.02 | 0.03 | - | 0.02 | 0.07 | $4.0 \\%$ |\n",
      "| Dinka Pharma | B2B Medicine Sales | - | 0.01 | 0.00 | - | - | 0.01 | $0.6 \\%$ |\n",
      "| Dia Plus | B2B Medicine Sales | - | - | - | - | 0.01 | 0.01 | $0.6 \\%$ |\n",
      "| Family Tree Pharma | B2B Medicine Sales | - | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | $0.5 \\%$ |\n",
      "| Pharma Bliss | B2B Medicine Sales | - | - | - | 0.00 | 0.01 | 0.01 | $0.4 \\%$ |\n",
      "| Bharath Medicals | B2B Medicine Sales | - | - | - | - | 0.05 | 0.05 | $2.9 \\%$ |\n",
      "| Beyond Health Care | B2B Medicine Sales | - | - | - | - | 0.05 | 0.05 | $2.6 \\%$ |\n",
      "| Others | Others | 0.05 | 0.01 | 0.01 | 0.00 | 0.45 | 0.53 | $28.8 \\%$ |\n",
      "| Total |  | 0.05 | 0.62 | 0.05 | 0.01 | 1.10 | 1.83 | $100.0 \\%$ |\n",
      "| Share \\% |  | $2.9 \\%$ | $33.8 \\%$ | $2.7 \\%$ | $0.4 \\%$ | $60.2 \\%$ | $100.0 \\%$ |  |\n",
      "\n",
      "Payables: Payables are mostly in the form payables to device man\n",
      "================================================================================\n",
      "Page 20 has a table-like structure:\n",
      "$>$ At home healthcare services including doctor, nurse, and physio on demand\n",
      "$>$ Specialized testing including genetic testing (gut microbiome, etc.)\n",
      "\n",
      "- Corporate Vertical\n",
      "$>$ Phable is building its corporate vertical to provide corporate health benefit plans targeted towards early-stage startups with an employee strength of 50-150 employees.\n",
      "$>$ These health benefit plans will be a combination of health insurance and healthcare services that can be availed on the Phable app. In the near future, the company also plans to roll out healthcare credit cards for employees which would allow them to seamlessly avail 36-month healthcare loans.\n",
      "$>$ Since its recent launch, Phable has onboarded 2 corporates including Ola. The company is in the process of hiring experienced senior professionals with a strong network of corporates to bolster its corporate partnerships.\n",
      "\n",
      "\n",
      "# Doctors and Clinics \n",
      "\n",
      "Currently, Phable is focusing on adoption and acquiring patients through doctors. Therefore, the platfo\n",
      "================================================================================\n",
      "Page 21 has a table-like structure:\n",
      "| eKincare | eKincare is an online employee health benefits platform. It helps patients to <br> manage their healthcare records. Based on the analytics, the company <br> provides personalized healthcare plans to individuals. | INR -20Cr <br> (FY21) |\n",
      "| :--: | :-- | :--: |\n",
      "| Afford Plan | Affordplan is a savings and financial planning platform for medical <br> expenses. Individuals can develop savings options, pick up services, <br> enrollment services, among others. The company also offers insights to <br> access patients' demographics, redeem benefits, and consultation services. | INR 7.2Cr <br> (FY20) |\n",
      "| Healthplix | Healthplix provides an electronic medical record solution for chronic care <br> management, with features such as e-prescription generation, lab <br> management, billing, etc. It also has dashboards providing AI \\& machine <br> learning-based insights related to finance, marketing, test clinical <br> hypothesis, and treatment outcomes and risk. The company is currently \n",
      "================================================================================\n",
      "Page 28 has a table-like structure:\n",
      "> Combining Brick-and-Mortar with Technology: With tier 2 and tier 3 cities facing shortages of primary care, brick-and-mortar health centres equipped with technology can bridge the service gaps. This has proven effective in delivering digital healthcare services to the underserved.\n",
      "\n",
      "Competition:\n",
      "Growth Startups\n",
      "\n",
      "| Year of incorporation | $\\begin{gathered} \\text { Phable } \\\\ 2017 \\end{gathered}$ | $\\begin{gathered} \\text { MediBuddy } \\\\ \\text { Year Death Buddy } \\end{gathered}$ | $\\begin{gathered} \\text { mfine } \\\\ 2017 \\end{gathered}$ | $\\begin{gathered} \\text { praceto } \\\\ 2008 \\end{gathered}$ | $\\begin{gathered} \\text { Eekincare } \\end{gathered}$ |\n",
      "| :--: | :--: | :--: | :--: | :--: | :--: |\n",
      "| Business Model | B2B2C | B2C, B2B2C | B2C, B2B2C | B2C | B2B2C |\n",
      "| Core Offerings | Telemedicine, Appointment Bookings | Telemedicine, Corporate health benefit programs | Telemedicine, Corporate health benefit programs | Appointment bookings, Telemedicine | Corporate health benefit progr\n",
      "================================================================================\n",
      "Page 29 has a table-like structure:\n",
      "# Financial Analysis \\& Projections \n",
      "\n",
      "Date of Incorporation: 2018\n",
      "Auditor: Nemani and Associates (Tier II Bangalore)\n",
      "Audit Opinion for FY20: Unqualified Opinion. FY21 report expected by $15^{\\text {th }}$ December, 2021 as it is a CP to the Equity infusion.\n",
      "\n",
      "Historical Analysis\n",
      "\n",
      "| Particulars <br> (INR Cr) | Apr-21 | May-21 | Jun-21 | Jul-21 | Aug-21 | Sep-21 | Oct-21 | FY22 <br> YTD |\n",
      "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
      "| GMV | 1.21 | 1.15 | 0.67 | 1.50 | 2.70 | 5.79 | 7.88 | 20.90 |\n",
      "| Revenue | 0.73 | 0.78 | 0.20 | 0.33 | 1.08 | 3.49 | 4.36 | 10.97 |\n",
      "| COGS | 0.30 | 0.37 | 0.19 | 0.19 | 0.40 | 1.53 | 2.11 | 5.09 |\n",
      "| Gross Margin | 0.42 | 0.41 | 0.01 | 0.14 | 0.69 | 1.96 | 2.24 | 5.88 |\n",
      "| Gross Margin \\% | $58 \\%$ | $53 \\%$ | $6 \\%$ | $42 \\%$ | $63 \\%$ | $56 \\%$ | $51 \\%$ | $54 \\%$ |\n",
      "| Ops Salary | 0.60 | 0.51 | 0.52 | 0.62 | 0.71 | 0.93 | 1.24 | 5.14 |\n",
      "| CM1 | $-0.17$ | $-0.09$ | $-0.51$ | $-0.48$ | $-0.02$ | 1.03 | 1.00 | 0.74 |\n",
      "| CM1 \\% | $-24 \\%$ | $-1\n",
      "================================================================================\n",
      "Page 30 has a table-like structure:\n",
      "| Interest Cost | 1.27 | 14.25 | 14.25 | 1.27 | 14.25 | 14.25 |\n",
      "| :-- | :--: | :--: | :--: | :--: | :--: | :--: |\n",
      "| One-time expenses | 2.56 | 6.50 | 6.50 | 2.56 | 6.50 | 6.50 |\n",
      "| PBT | -65.72 | -224.29 | 2.81 | -76.25 | -138.58 | -78.11 |\n",
      "| PBT \\% | $-75 \\%$ | $-30 \\%$ | $0 \\%$ | $-134 \\%$ | $-43 \\%$ | $-10 \\%$ |\n",
      "\n",
      "# Balance Sheet: \n",
      "\n",
      "| Particulars | FY21 | Management Case |  |  | Stride Case |  |  |\n",
      "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
      "|  |  | FY22 | FY23 | FY24 | FY22 | FY23 | FY24 |\n",
      "| LIABILITIES |  |  |  |  |  |  |  |\n",
      "| Net worth | 39.9 | 200.5 | 713.4 | 703.5 | 191.0 | 414.6 | 350.4 |\n",
      "| Long Term Borrowings | - | 45.0 | 150.0 | 125.0 | 45.0 | 95.0 | 75.0 |\n",
      "| Current Liabilities | 1.4 | 2.5 | 5.5 | 10.8 | 2.5 | 5.5 | 10.8 |\n",
      "| Other Payables | 0.4 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 |\n",
      "| TOTAL | 41.7 | 249.0 | 869.9 | 840.2 | 239.5 | 516.1 | 437.2 |\n",
      "| ASSETS |  |  |  |  |  |  |  |\n",
      "| Fixed Assets | 2.8 | 3.0 | 3.8 | 4.5 | 3.0 | 3.8 | 4.5 |\n",
      "| Current Assets |  |  | \n",
      "================================================================================\n",
      "Page 31 has a table-like structure:\n",
      "Indicative Ratios:\n",
      "\n",
      "|  |  | Management Case |  |  |  |  |  | Stride Case |  |  |  |  |\n",
      "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
      "| Particulars |  | FY21 | FY22 | FY23 |  | FY24 | FY22 | FY23 | FY24 |  |  |  |\n",
      "| Total Debt/NW |  | $0.0 \\%$ | $22.4 \\%$ | $21.2 \\%$ |  | $18.0 \\%$ | $23.6 \\%$ | $22.9 \\%$ | $21.4 \\%$ |  |  |  |\n",
      "| Total Debt/Equity raised till date |  | $0.0 \\%$ | $18.9 \\%$ | $20.0 \\%$ |  | - | $24.0 \\%$ | $25.3 \\%$ | - |  |  |  |\n",
      "| Total Debt/Valuation |  | $0.0 \\%$ | $5.0 \\%$ | $4.7 \\%$ |  | - | $5.0 \\%$ | $5.9 \\%$ | - |  |  |  |\n",
      "| Current Ratio (ex-cash) |  | 1.5 | 1.2 | 1.1 |  | 1.1 | 1.2 | 1.1 | 1.1 |  |  |  |\n",
      "| Months of runway |  | 12 | 16 | $>24$ |  | $>24$ | 14 | $>24$ | $>24$ |  |  |  |\n",
      "\n",
      "Working Capital:\n",
      "\n",
      "|  |  | Management Case |  |  |  |  |  |  | Stride Case |  |  |  |  |\n",
      "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
      "| Particulars (INR Cr) | FY21 |  | FY22 |  \n",
      "================================================================================\n",
      "Page 32 has a table-like structure:\n",
      "|  |  | Exit Revenue INR Cr (MRR) |  |  |  |\n",
      "| :-- | :-- | :--: | :--: | :--: | :--: |\n",
      "|  | Financial year | $\\mathbf{6 0 0}$ | $\\mathbf{8 0 0}$ | $\\mathbf{1 0 0 0}$ | $\\mathbf{1 2 0 0}$ |\n",
      "| Year of Exercise <br> post Maturity | Jun-25 | $31.6 \\%$ | $35.6 \\%$ | $38.8 \\%$ | $38.8 \\%$ |\n",
      "|  | Jun-26 | $28.1 \\%$ | $31.2 \\%$ | $33.7 \\%$ | $33.7 \\%$ |\n",
      "|  | Jun-27 | $25.6 \\%$ | $28.1 \\%$ | $30.1 \\%$ | $30.1 \\%$ |\n",
      "\n",
      "Risk and Mitigation\n",
      "\n",
      "| Risk | Mitigation |\n",
      "| :--: | :--: |\n",
      "| High burn and no clear path to profitability <br> On a monthly GMV of $\\sim$ INR 8 Cr , the company has a monthly burn of $\\sim$ INR 6.8 Cr (Oct'21) due to discounts and customer acquisition costs | $>$ Phable is focusing strongly on customer acquisition and retention which has resulted in a high monthly burn of $\\sim$ INR 6.7 Cr . However, the company is adding over 1.5 lakh users each month and has added $\\sim 6$ lakh users in the last 6 month through a successful customer acquisition and marketing strategy. <br> Phable \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "for page in ocr_data[\"pages\"]:\n",
    "    markdown_text = page[\"markdown\"]\n",
    "    if \"|\" in markdown_text:  # Check if table-like structure exists\n",
    "        print(f\"Page {page['index']} has a table-like structure:\")\n",
    "        print(markdown_text[:1000])  # Print first 1000 characters to inspect\n",
    "        print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 tables in the JSON file\n",
      "Removed existing file: extracted_tables.xlsx\n",
      "Prepared table 1 from page 0\n",
      "Prepared table 2 from page 1\n",
      "Prepared table 3 from page 1\n",
      "Prepared table 4 from page 2\n",
      "Prepared table 5 from page 4\n",
      "Prepared table 6 from page 5\n",
      "Prepared table 7 from page 5\n",
      "Prepared table 8 from page 6\n",
      "Prepared table 9 from page 7\n",
      "Prepared table 10 from page 12\n",
      "Prepared table 11 from page 13\n",
      "Prepared table 12 from page 13\n",
      "Prepared table 13 from page 13\n",
      "Prepared table 14 from page 14\n",
      "Prepared table 16 from page 16\n",
      "Prepared table 17 from page 17\n",
      "Prepared table 18 from page 17\n",
      "Prepared table 19 from page 18\n",
      "Prepared table 21 from page 19\n",
      "Prepared table 22 from page 19\n",
      "Prepared table 24 from page 21\n",
      "Prepared table 25 from page 21\n",
      "Prepared table 26 from page 28\n",
      "Prepared table 27 from page 29\n",
      "Prepared table 28 from page 29\n",
      "Prepared table 29 from page 30\n",
      "Prepared table 30 from page 30\n",
      "Prepared table 31 from page 30\n",
      "Prepared table 32 from page 31\n",
      "Prepared table 33 from page 31\n",
      "Prepared table 34 from page 31\n",
      "Prepared table 35 from page 32\n",
      "Prepared table 36 from page 32\n",
      "\n",
      "Successfully saved 33 tables to extracted_tables.xlsx\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def extract_tables_from_json(json_file_path):\n",
    "    # Load the JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # List to store all tables\n",
    "    all_tables = []\n",
    "    \n",
    "    # Function to extract tables from markdown text\n",
    "    def extract_markdown_tables(text, page_num=None):\n",
    "        # Pattern to match markdown tables\n",
    "        table_pattern = r'(\\|.*\\|[\\s]*\\n\\|[\\s]*:?-+:?[\\s]*\\|.*\\n(?:\\|.*\\|[\\s]*\\n)*)'\n",
    "        \n",
    "        # Find all tables in the text\n",
    "        tables = re.findall(table_pattern, text)\n",
    "        \n",
    "        # Add page number information if available\n",
    "        if page_num is not None:\n",
    "            return [(table, page_num) for table in tables]\n",
    "        return [(table, None) for table in tables]\n",
    "    \n",
    "    # Check if the JSON has a pages structure\n",
    "    if isinstance(data, dict) and 'pages' in data:\n",
    "        for i, page in enumerate(data['pages']):\n",
    "            if 'markdown' in page:\n",
    "                tables = extract_markdown_tables(page['markdown'], i)\n",
    "                all_tables.extend(tables)\n",
    "    # Alternative structure: list of pages\n",
    "    elif isinstance(data, list) and data and isinstance(data[0], dict) and 'markdown' in data[0]:\n",
    "        for i, page in enumerate(data):\n",
    "            if 'markdown' in page:\n",
    "                tables = extract_markdown_tables(page['markdown'], i)\n",
    "                all_tables.extend(tables)\n",
    "    # If it's not in the expected structure, try to search in any string field\n",
    "    else:\n",
    "        def search_in_obj(obj, path=\"\"):\n",
    "            tables = []\n",
    "            if isinstance(obj, dict):\n",
    "                for key, value in obj.items():\n",
    "                    new_path = f\"{path}.{key}\" if path else key\n",
    "                    if key == \"markdown\" and isinstance(value, str):\n",
    "                        # Extract page number if possible\n",
    "                        page_match = re.search(r'page[_\\s]*(\\d+)', new_path, re.IGNORECASE)\n",
    "                        page_num = int(page_match.group(1)) if page_match else None\n",
    "                        tables.extend(extract_markdown_tables(value, page_num))\n",
    "                    elif isinstance(value, (dict, list)):\n",
    "                        tables.extend(search_in_obj(value, new_path))\n",
    "            elif isinstance(obj, list):\n",
    "                for i, item in enumerate(obj):\n",
    "                    tables.extend(search_in_obj(item, f\"{path}[{i}]\"))\n",
    "            return tables\n",
    "        \n",
    "        all_tables = search_in_obj(data)\n",
    "    \n",
    "    return all_tables\n",
    "\n",
    "def clean_cell_content(cell_text):\n",
    "    # Remove non-printable characters and control characters\n",
    "    clean_text = ''.join(char for char in cell_text if ord(char) >= 32 or char in '\\n\\t')\n",
    "    # Remove excessive whitespace\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    return clean_text\n",
    "\n",
    "def parse_markdown_table(table_text):\n",
    "    # Split the table into lines\n",
    "    lines = table_text.strip().split('\\n')\n",
    "    \n",
    "    # Find the header row and separator row\n",
    "    header_row = None\n",
    "    separator_row = None\n",
    "    data_rows = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if not line.startswith('|') or not line.endswith('|'):\n",
    "            continue\n",
    "            \n",
    "        if separator_row is None and (':--' in line or '---' in line):\n",
    "            separator_row = i\n",
    "            header_row = i - 1 if i > 0 else None\n",
    "        elif separator_row is not None:\n",
    "            data_rows.append(i)\n",
    "    \n",
    "    # If we didn't find a separator row, try a simpler approach\n",
    "    if separator_row is None:\n",
    "        if len(lines) >= 2:\n",
    "            header_row = 0\n",
    "            data_rows = list(range(1, len(lines)))\n",
    "    \n",
    "    # Process rows\n",
    "    processed_rows = []\n",
    "    \n",
    "    # Process all rows (including header if present)\n",
    "    rows_to_process = ([header_row] if header_row is not None else []) + data_rows\n",
    "    \n",
    "    for i in rows_to_process:\n",
    "        if i is None or i >= len(lines):\n",
    "            continue\n",
    "            \n",
    "        line = lines[i].strip()\n",
    "        if not line.startswith('|') or not line.endswith('|'):\n",
    "            continue\n",
    "            \n",
    "        # Remove leading and trailing pipes and split by pipe\n",
    "        cells = line.strip('|').split('|')\n",
    "        # Clean up whitespace and remove non-printable characters in each cell\n",
    "        cells = [clean_cell_content(cell) for cell in cells]\n",
    "        processed_rows.append(cells)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if processed_rows:\n",
    "        if header_row is not None and len(processed_rows) > 0:\n",
    "            headers = processed_rows[0]\n",
    "            data = processed_rows[1:] if len(processed_rows) > 1 else []\n",
    "        else:\n",
    "            # Create default headers\n",
    "            if processed_rows:\n",
    "                max_cols = max(len(row) for row in processed_rows)\n",
    "                headers = [f\"Column{i+1}\" for i in range(max_cols)]\n",
    "                data = processed_rows\n",
    "            else:\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "        # Clean header names\n",
    "        headers = [clean_cell_content(h) if h else f\"Column{i+1}\" for i, h in enumerate(headers)]\n",
    "        \n",
    "        # Ensure all rows have the same number of columns\n",
    "        max_cols = len(headers)\n",
    "        for i in range(len(data)):\n",
    "            if len(data[i]) < max_cols:\n",
    "                data[i].extend([''] * (max_cols - len(data[i])))\n",
    "            elif len(data[i]) > max_cols:\n",
    "                data[i] = data[i][:max_cols]\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        return df\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "def save_tables_to_excel(tables, output_excel_path):\n",
    "    # Remove file if it already exists to avoid issues\n",
    "    if os.path.exists(output_excel_path):\n",
    "        try:\n",
    "            os.remove(output_excel_path)\n",
    "            print(f\"Removed existing file: {output_excel_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not remove existing file: {e}\")\n",
    "    \n",
    "    # Create a list to store DataFrames with their sheet names\n",
    "    dfs_to_save = []\n",
    "    tables_saved = 0\n",
    "    \n",
    "    # First, parse all tables and prepare them\n",
    "    for i, (table, page_num) in enumerate(tables):\n",
    "        df = parse_markdown_table(table)\n",
    "        \n",
    "        if not df.empty and len(df) > 0:\n",
    "            # Create a sheet name with page information if available\n",
    "            sheet_name = f\"Page{page_num}_Table{i+1}\" if page_num is not None else f\"Table_{i+1}\"\n",
    "            \n",
    "            # Excel sheet names must be <= 31 characters\n",
    "            if len(sheet_name) > 31:\n",
    "                sheet_name = sheet_name[:31]\n",
    "            \n",
    "            # Store the DataFrame and its sheet name\n",
    "            dfs_to_save.append((df, sheet_name))\n",
    "            tables_saved += 1\n",
    "            print(f\"Prepared table {i+1}\" + (f\" from page {page_num}\" if page_num is not None else \"\"))\n",
    "    \n",
    "    # Now save all prepared DataFrames to Excel\n",
    "    if dfs_to_save:\n",
    "        try:\n",
    "            with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:\n",
    "                used_sheet_names = set()\n",
    "                \n",
    "                for df, sheet_name in dfs_to_save:\n",
    "                    # Make sure sheet name is unique\n",
    "                    original_name = sheet_name\n",
    "                    counter = 1\n",
    "                    while sheet_name in used_sheet_names:\n",
    "                        sheet_name = f\"{original_name[:27]}_{counter}\"\n",
    "                        counter += 1\n",
    "                    \n",
    "                    used_sheet_names.add(sheet_name)\n",
    "                    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                    \n",
    "            print(f\"\\nSuccessfully saved {tables_saved} tables to {output_excel_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving Excel file: {e}\")\n",
    "            \n",
    "            # Alternative approach - save each table as a separate CSV file\n",
    "            print(\"Attempting to save tables as individual CSV files instead...\")\n",
    "            csv_folder = os.path.splitext(output_excel_path)[0] + \"_csv\"\n",
    "            os.makedirs(csv_folder, exist_ok=True)\n",
    "            \n",
    "            for i, (df, sheet_name) in enumerate(dfs_to_save):\n",
    "                safe_name = re.sub(r'[^\\w\\-_]', '_', sheet_name)\n",
    "                csv_path = os.path.join(csv_folder, f\"{safe_name}.csv\")\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                print(f\"Saved table to {csv_path}\")\n",
    "                \n",
    "            print(f\"Saved {len(dfs_to_save)} tables as CSV files in {csv_folder}\")\n",
    "    else:\n",
    "        print(\"No tables to save.\")\n",
    "    \n",
    "    return tables_saved\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    json_file_path = 'ocr_response.json'  # Replace with your JSON file path\n",
    "    output_excel_path = 'extracted_tables.xlsx'\n",
    "    \n",
    "    # Extract tables\n",
    "    tables = extract_tables_from_json(json_file_path)\n",
    "    \n",
    "    # Check if tables were found\n",
    "    if tables:\n",
    "        print(f\"Found {len(tables)} tables in the JSON file\")\n",
    "        tables_saved = save_tables_to_excel(tables, output_excel_path)\n",
    "        if tables_saved == 0:\n",
    "            print(\"No valid tables could be parsed from the found content\")\n",
    "    else:\n",
    "        print(\"No markdown tables found in the JSON file\")\n",
    "        \n",
    "        # Debug: Let's print some information about the JSON structure\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "        print(\"\\nJSON structure information for debugging:\")\n",
    "        if isinstance(data, dict):\n",
    "            print(f\"Top-level keys: {', '.join(data.keys())}\")\n",
    "            if 'pages' in data and isinstance(data['pages'], list):\n",
    "                print(f\"Contains {len(data['pages'])} pages\")\n",
    "                if data['pages'] and isinstance(data['pages'][0], dict):\n",
    "                    print(f\"First page keys: {', '.join(data['pages'][0].keys())}\")\n",
    "        elif isinstance(data, list):\n",
    "            print(f\"JSON contains a list with {len(data)} items\")\n",
    "            if data and isinstance(data[0], dict):\n",
    "                print(f\"First item keys: {', '.join(data[0].keys())}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 29 images. Saved in 'outputs/images/'\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "# Create output directory for images\n",
    "os.makedirs(\"outputs/images\", exist_ok=True)\n",
    "\n",
    "image_count = 0\n",
    "\n",
    "for page in ocr_data[\"pages\"]:\n",
    "    if \"images\" in page:\n",
    "        for image in page[\"images\"]:\n",
    "            if \"image_base64\" in image:\n",
    "                image_data = base64.b64decode(image[\"image_base64\"].split(\",\")[1])\n",
    "                image_path = f\"outputs/images/{image['id']}\"\n",
    "                \n",
    "                # Save the image\n",
    "                with open(image_path, \"wb\") as img_file:\n",
    "                    img_file.write(image_data)\n",
    "\n",
    "                # Verify the image integrity\n",
    "                try:\n",
    "                    with Image.open(image_path) as img:\n",
    "                        img.verify()  # Ensure it's a valid image\n",
    "                    #print(f\"Saved: {image_path}\")\n",
    "                    image_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Corrupt Image Skipped: {image['id']} - {e}\")\n",
    "\n",
    "print(f\"Extracted {image_count} images. Saved in 'outputs/images/'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
