{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images saved at: ['E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_1.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_2.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_3.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_4.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_5.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_6.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_7.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_8.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_9.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_10.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_11.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_12.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_13.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_14.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_15.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_16.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_17.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_18.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_19.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_20.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_21.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_22.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_23.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_24.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_25.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_26.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_27.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_28.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_29.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_30.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_31.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_32.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_33.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_34.png', 'E:\\\\Btech_AI\\\\Intern\\\\ocrpro\\\\LLM Approach\\\\results\\\\Llama results\\\\page_35.png']\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "# Convert PDF to images\n",
    "def pdf_to_images(pdf_path, output_dir):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        pix = page.get_pixmap()\n",
    "        \n",
    "        # Save image to specified directory\n",
    "        image_path = os.path.join(output_dir, f\"page_{page_num+1}.png\")\n",
    "        pix.save(image_path)\n",
    "        images.append(image_path)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Define paths\n",
    "pdf_path = r\"E:\\Btech_AI\\Intern\\ocrpro\\Phable CAM Final.pdf\"\n",
    "output_dir = r\"E:\\Btech_AI\\Intern\\ocrpro\\LLM Approach\\results\\Llama results\"\n",
    "\n",
    "# Convert PDF to images\n",
    "images = pdf_to_images(pdf_path, output_dir)\n",
    "\n",
    "print(\"Images saved at:\", images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chira\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\chira\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    import pdfplumber\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "text = extract_text_from_pdf(r\"E:\\Btech_AI\\Intern\\ocrpro\\Phable CAM Final.pdf\")\n",
    "\n",
    "# Split text into sentences\n",
    "sentences = text.split(\".\")\n",
    "\n",
    "# Generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Cluster sentences into sections\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Group sentences by cluster\n",
    "sections = {}\n",
    "for i, cluster in enumerate(clusters):\n",
    "    if cluster not in sections:\n",
    "        sections[cluster] = []\n",
    "    sections[cluster].append(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Example API call to LLaMA 3.2 Vision API\n",
    "def extract_data_with_llama(image_path):\n",
    "    url = \"https://api.llama.ai/v1/vision\"\n",
    "    headers = {\"Authorization\": \"Bearer YOUR_API_KEY\"}\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        response = requests.post(url, headers=headers, files={\"image\": image_file})\n",
    "    return response.json()\n",
    "\n",
    "# Extract data from an image section\n",
    "data = extract_data_with_llama(\"page_1.png\")\n",
    "print(data) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
